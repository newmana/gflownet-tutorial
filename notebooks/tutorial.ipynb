{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Generative Flow Network Demo\n",
    "From https://colab.research.google.com/drive/1fUMwgu2OhYpQagpzU5mhe9_Esib3Q2VR\n"
   ],
   "id": "f78bc92c4b823720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as matplotlib_pyplot\n",
    "import numpy as numpy\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "import gflownet as gflownet"
   ],
   "id": "8c08801e5eba6378",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_faces(faces):\n",
    "    f, ax = matplotlib_pyplot.subplots(1, len(faces))\n",
    "    for i in range(len(faces)):\n",
    "        if len(faces) > 1:\n",
    "            matplotlib_pyplot.sca(ax[i])\n",
    "        gflownet.Face.draw_face(faces[i])\n",
    "\n",
    "\n",
    "smiling_face = gflownet.Face(['smile', 'left_eb_down', 'right_eb_down'])\n",
    "frowning_face = gflownet.Face(['frown', 'left_eb_up', 'right_eb_up'])\n",
    "plot_faces([gflownet.Face(['left_eb_up', 'right_eb_up']), gflownet.Face(['frown']), gflownet.Face(['smile'])])\n",
    "plot_faces([gflownet.Face(['left_eb_up', 'left_eb_down']), gflownet.Face(['right_eb_up', 'right_eb_down']),\n",
    "            gflownet.Face(['left_eb_up', 'left_eb_down', 'right_eb_up', 'right_eb_down']),\n",
    "            gflownet.Face(['smile', 'frown'])])\n",
    "plot_faces([frowning_face, smiling_face])"
   ],
   "id": "ff575fa92ad2c888",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![Differing representations of invalid faces, frowning and smiling face](./images/faces_1.png)\n",
    "<br>\n",
    "![Differing representations of invalid faces, frowning and smiling face](./images/faces_2.png)\n",
    "<br>\n",
    "![Differing representations of invalid faces, frowning and smiling face](./images/faces_3.png)"
   ],
   "id": "3d6503b03f2786f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "enumerated_states, transitions = gflownet.Face.enumerate_states_transitions(gflownet.Face.sorted_keys)\n",
    "unique_states = []\n",
    "for face in enumerated_states:\n",
    "    if set(face.patches) not in [set(u.patches) for u in unique_states]:\n",
    "        unique_states.append(face)"
   ],
   "id": "a0b7d592ec97a676",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gflownet.Network.plot(unique_states, transitions)",
   "id": "4463cf6168a14253",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![All possible faces - state space](./images/state_space.png)",
   "id": "87104d84b99b8893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flow Matching\n",
    "https://arxiv.org/abs/2106.04399"
   ],
   "id": "1441d296cda541e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def face_parents(state):\n",
    "    parent_states = []  # states that are parents of state\n",
    "    parent_actions = []  # actions that lead from those parents to state\n",
    "    for face_part in state:\n",
    "        # For each face part, there is a parent without that part\n",
    "        parent_states.append([i for i in state if i != face_part])\n",
    "        # The action to get there is the corresponding index of that face part\n",
    "        parent_actions.append(gflownet.Face.sorted_keys.index(face_part))\n",
    "    return parent_states, parent_actions"
   ],
   "id": "e82f5bc509396439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate model and optimizer\n",
    "flow_matching_model = gflownet.FlowModel(512)\n",
    "opt = torch.optim.Adam(flow_matching_model.parameters(), 3e-4)\n",
    "\n",
    "# Let's keep track of the losses and the faces we sample\n",
    "losses = []\n",
    "sampled_faces = []\n",
    "# To not complicate the code, I'll just accumulate losses here and take a\n",
    "# gradient step every `update_freq` episode.\n",
    "minibatch_loss = 0\n",
    "update_freq = 4\n",
    "for episode in tqdm.tqdm(range(50000), ncols=40):\n",
    "    # Each episode starts with an \"empty state\"\n",
    "    state = []\n",
    "    # Predict F(s, a)\n",
    "    edge_flow_prediction = flow_matching_model(gflownet.Face(state).face_to_tensor())\n",
    "    for t in range(3):\n",
    "        # The policy is just normalizing, and gives us the probability of each action\n",
    "        policy = edge_flow_prediction / edge_flow_prediction.sum()\n",
    "        # Sample the action\n",
    "        action = Categorical(probs=policy).sample()\n",
    "        # \"Go\" to the next state\n",
    "        new_state = state + [gflownet.Face.sorted_keys[action]]\n",
    "\n",
    "        # Now we want to compute the loss, we'll first enumerate the parents\n",
    "        parent_states, parent_actions = face_parents(new_state)\n",
    "        # And compute the edge flows F(s, a) of each parent\n",
    "        px = torch.stack([gflownet.Face(p).face_to_tensor() for p in parent_states])\n",
    "        pa = torch.tensor(parent_actions).long()\n",
    "        parent_edge_flow_preds = flow_matching_model(px)[torch.arange(len(parent_states)), pa]\n",
    "        # Now we need to compute the reward and F(s, a) of the current state,\n",
    "        # which is currently `new_state`\n",
    "        if t == 2:\n",
    "            # If we've built a complete face, we're done, so the reward is > 0\n",
    "            # (unless the face is invalid)\n",
    "            reward = gflownet.Face(new_state).face_reward()\n",
    "            # and since there are no children to this state F(s,a) = 0 \\forall a\n",
    "            edge_flow_prediction = torch.zeros(6)\n",
    "        else:\n",
    "            # Otherwise we keep going, and compute F(s, a)\n",
    "            reward = 0\n",
    "            edge_flow_prediction = flow_matching_model(gflownet.Face(new_state).face_to_tensor())\n",
    "\n",
    "        # The loss as per the equation above\n",
    "        flow_mismatch = (parent_edge_flow_preds.sum() - edge_flow_prediction.sum() - reward).pow(2)\n",
    "        minibatch_loss += flow_mismatch  # Accumulate\n",
    "        # Continue iterating\n",
    "        state = new_state\n",
    "\n",
    "    # We're done with the episode, add the face to the list, and if we are at an\n",
    "    # update episode, take a gradient step.\n",
    "    sampled_faces.append(gflownet.Face(state))\n",
    "    if episode % update_freq == 0:\n",
    "        losses.append(minibatch_loss.item())\n",
    "        minibatch_loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        minibatch_loss = 0"
   ],
   "id": "bd396abc4fad6003",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "matplotlib_pyplot.figure(figsize=(10, 3))\n",
    "matplotlib_pyplot.plot(losses)\n",
    "matplotlib_pyplot.yscale('log')"
   ],
   "id": "1fcbda625be37f37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Flow Matching Loss over Training](./images/flow_matching_loss.png)",
   "id": "c1849156c10d9898"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f, ax = matplotlib_pyplot.subplots(8, 8, figsize=(4, 4))\n",
    "print('Ratio of faces with a smile:', sum(['smile' in i.patches for i in sampled_faces[-128:]]) / 128)\n",
    "print('Ratio of valid faces:', sum([i.face_reward() > 0 for i in sampled_faces[-128:]]) / 128)\n",
    "for i, face in enumerate(sampled_faces[-64:]):\n",
    "    matplotlib_pyplot.sca(ax[i // 8, i % 8])\n",
    "    face.draw_face()"
   ],
   "id": "d6a6d25dca1d332c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Samples Faces for Flow Matching](./images/flow_matching_faces.png)",
   "id": "5dd6cf482d05cd0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gflownet.Network.plot(unique_states, transitions, flow_matching_model)",
   "id": "daaee368adf6d2eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![All Edge Flows from Starting Face](./images/flows.png)",
   "id": "4594d4ef28b9778f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "flow_matching_model(gflownet.Face([]).face_to_tensor()).sum()",
   "id": "445fee643e0d71bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Trajectory Balancing\n",
    "\n",
    "https://arxiv.org/abs/2201.13259"
   ],
   "id": "41117d2ef80b2479"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate model and optimizer\n",
    "tb_model = gflownet.TBModel(512)\n",
    "opt = torch.optim.Adam(tb_model.parameters(), 3e-4)\n",
    "\n",
    "# Let's keep track of the losses and the faces we sample\n",
    "tb_losses = []\n",
    "tb_sampled_faces = []\n",
    "# To not complicate the code, I'll just accumulate losses here and take a\n",
    "# gradient step every `update_freq` episode.\n",
    "minibatch_loss = 0\n",
    "update_freq = 2\n",
    "\n",
    "logZs = []\n",
    "for episode in tqdm.tqdm(range(50000), ncols=40):\n",
    "    # Each episode starts with an \"empty state\"\n",
    "    state = []\n",
    "    # Predict P_F, P_B\n",
    "    prob_forward, prob_backward = tb_model(gflownet.Face(state).face_to_tensor())\n",
    "    total_prob_forward = 0\n",
    "    total_prob_backward = 0\n",
    "    for t in range(3):\n",
    "        # Here P_F is logits, so we want the Categorical to compute the softmax for us\n",
    "        cat = Categorical(logits=prob_forward)\n",
    "        action = cat.sample()\n",
    "        # \"Go\" to the next state\n",
    "        new_state = state + [gflownet.Face.sorted_keys[action]]\n",
    "        # Accumulate the P_F sum\n",
    "        total_prob_forward += cat.log_prob(action)\n",
    "\n",
    "        if t == 2:\n",
    "            # If we've built a complete face, we're done, so the reward is > 0\n",
    "            # (unless the face is invalid)\n",
    "            reward = torch.tensor(gflownet.Face(new_state).face_reward()).float()\n",
    "        # We recompute P_F and P_B for new_state\n",
    "        prob_forward, prob_backward = tb_model(gflownet.Face(new_state).face_to_tensor())\n",
    "        # Here we accumulate P_B, going backwards from `new_state`. We're also just\n",
    "        # going to use opposite semantics for the backward policy. I.e., for P_F action\n",
    "        # `i` just added the face part `i`, for P_B we'll assume action `i` removes\n",
    "        # face part `i`, this way we can just keep the same indices.\n",
    "        total_prob_backward += Categorical(logits=prob_backward).log_prob(action)\n",
    "\n",
    "        # Continue iterating\n",
    "        state = new_state\n",
    "\n",
    "    # We're done with the trajectory, let's compute its loss. Since the reward can\n",
    "    # sometimes be zero, instead of log(0) we'll clip the log-reward to -20.\n",
    "    loss = (tb_model.logZ + total_prob_forward - torch.log(reward).clip(-20) - total_prob_backward).pow(2)\n",
    "    minibatch_loss += loss\n",
    "\n",
    "    # Add the face to the list, and if we are at an\n",
    "    # update episode, take a gradient step.\n",
    "    tb_sampled_faces.append(state)\n",
    "    if episode % update_freq == 0:\n",
    "        tb_losses.append(minibatch_loss.item())\n",
    "        minibatch_loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        minibatch_loss = 0\n",
    "        logZs.append(tb_model.logZ.item())"
   ],
   "id": "e417f9774edb177f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f, ax = matplotlib_pyplot.subplots(2, 1, figsize=(10, 6))\n",
    "matplotlib_pyplot.sca(ax[0])\n",
    "matplotlib_pyplot.plot(tb_losses)\n",
    "matplotlib_pyplot.yscale('log')\n",
    "matplotlib_pyplot.ylabel('loss')\n",
    "matplotlib_pyplot.sca(ax[1])\n",
    "matplotlib_pyplot.plot(numpy.exp(logZs))\n",
    "matplotlib_pyplot.ylabel('estimated Z');"
   ],
   "id": "2b022d401c911a3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Trajectory Balancing Loss and Z Estimation](./images/trajectory_balancing_loss.png)",
   "id": "7e3c5e17083bdf03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f, ax = matplotlib_pyplot.subplots(8, 8, figsize=(4, 4))\n",
    "print('Ratio of faces with a smile:', sum(['smile' in i for i in tb_sampled_faces[-128:]]) / 128)\n",
    "print('Ratio of valid faces:', sum([gflownet.Face(i).face_reward() > 0 for i in tb_sampled_faces[-128:]]) / 128)\n",
    "for i, face in enumerate(tb_sampled_faces[-64:]):\n",
    "    matplotlib_pyplot.sca(ax[i // 8, i % 8])\n",
    "    gflownet.Face(face).draw_face()"
   ],
   "id": "cda5d473a08ffcf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Sampled Faces for Trajectory Balancing](./images/trajectory_balancing_faces.png)",
   "id": "e3c673ca7d9f4202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tb_model.logZ.exp()",
   "id": "112ac6a825807c2e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
